version: '3.8'

services:
  
  llamacpp:
    image: parsa1601/llamacpp-cuda:latest
    restart: unless-stopped
    ports:
      - "8080:8080"
    
    volumes:
      #- ./models:/models
      - $MODEL_PATH$:/models
      
    working_dir: /built-llama/build/bin
    
    command:
      - "./llama-server"
      - "-m"
      - "/models/merged_model_dare_q4_K_M.gguf"
      - "--mmproj"
      - "/models/mmproj-model-f16.gguf"
      - "-ngl"
      - "35"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all